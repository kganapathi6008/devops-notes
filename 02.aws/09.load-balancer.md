# Load Balancer and Regions vs Availability Zones (AZs)

## Region
A **Region** is a large geographical area containing multiple data centers. Example: `us-east-1`.

## Availability Zones (AZs)
**Availability Zones (AZs)** are separate, isolated data centers within a Region. Example: `us-east-1a`, `us-east-1b`, `us-east-1c`.

## Load Balancer Behavior
When using a **Load Balancer** in AWS:

- It can **distribute traffic across EC2 instances** located in **different AZs**.
- This **ensures high availability and fault tolerance** because if one AZ fails, traffic can be routed to instances in other AZs.
- **All AZs must be within the same Region** (e.g., `us-east-1`).
- **A Load Balancer cannot balance traffic across different Regions** (e.g., between `us-east-1` and `us-west-1`).

## Example
1. Launch EC2 instances in the following AZs within the same Region `us-east-1`:
   - `us-east-1a`
   - `us-east-1b`
   - `us-east-1c`

2. Create a **Load Balancer** in the Region `us-east-1`.

3. The Load Balancer will **distribute incoming traffic** across EC2 instances in `us-east-1a`, `us-east-1b`, and `us-east-1c`.

4. **However, the Load Balancer will not route traffic to instances in another Region like `us-west-1`**.

This behavior is important to understand when designing for **high availability** and **disaster recovery** in AWS.

---

# Layers in Load Balancing
Load balancing can occur at different layers of the OSI model. The most common are:

## Layer 4 Load Balancing (Transport Layer)
- Operates at the **Transport Layer (TCP/UDP)**.
- Makes routing decisions based on **IP addresses and ports**.
- Does **not inspect application content**.
- Faster because it works with lower-level network information.
- Examples: **AWS Network Load Balancer (NLB)**, **HAProxy (TCP mode)**.

## Layer 7 Load Balancing (Application Layer)
- Operates at the **Application Layer (HTTP/HTTPS)**.
- Understands application-level protocols like **HTTP**.
- Can make routing decisions based on **URLs, headers, cookies, or request content**.
- Supports **advanced traffic routing** such as path-based and host-based routing.
- Examples: **AWS Application Load Balancer (ALB)**, **NGINX (HTTP mode)**, **Apache HTTP Server**.

## Key Differences Between Layer 4 and Layer 7 Load Balancing
| Feature                    | Layer 4 (Transport Layer) | Layer 7 (Application Layer) |
|-----------------------------|-----------------------------|------------------------------|
| Protocol Level              | TCP/UDP                    | HTTP/HTTPS                   |
| Traffic Inspection          | No                         | Yes                          |
| Performance                 | High                       | Moderate                     |
| Routing Flexibility         | Basic                      | Advanced                     |
| Example Use Case            | Simple traffic forwarding  | Content-based routing        |

---

# OSI Model and its 7 Layers
The **OSI (Open Systems Interconnection) model** is a conceptual framework used to understand network communication. It has **7 layers**, each with a specific role:

| Layer Number | Layer Name         | Description                                      |
|--------------|---------------------|--------------------------------------------------|
| 7            | Application         | User interfaces and applications (HTTP, FTP)    |
| 6            | Presentation        | Data formatting, encryption, compression        |
| 5            | Session             | Manages sessions between devices                |
| 4            | Transport           | Reliable data delivery (TCP, UDP)               |
| 3            | Network             | Routing, IP addresses                           |
| 2            | Data Link           | MAC addresses, switches                         |
| 1            | Physical            | Physical hardware (cables, NICs)                |

---

# Web Servers and Application Servers
## Web Servers
Web servers handle HTTP requests from clients (e.g., browsers). They serve static content like HTML, CSS, and images. Popular web servers include:

- **Apache HTTP Server (httpd)**
- **NGINX**
- **HAProxy**

## Application Servers
Application servers handle dynamic content and business logic. They often run Java applications and interact with databases. Examples include:

- **Apache Tomcat**
- **JBoss (WildFly)**
- **Oracle WebLogic**
- **IBM WebSphere**

## Reverse Proxy with Web Servers
Web servers like **NGINX** can also act as a **reverse proxy**. This means they can:

- Receive client requests.
- Forward the requests to backend servers (e.g., application servers or multiple web servers).
- Balance the load between multiple backend servers.
- Return the response from backend servers to the client.

## Example: Load Balancing with NGINX
Assuming you have two web servers serving the same content:

| Server Name  | IP Address     |
| ------------ | -------------- |
| Web Server 1 | `172.31.22.151` |
| Web Server 2 | `172.31.22.152` |

### 1. Install NGINX
```bash
sudo apt update
sudo apt install nginx -y
sudo systemctl start nginx
sudo systemctl enable nginx
sudo systemctl stop nginx
```

### 2. [Configure NGINX as a Load Balancer](https://nginx.org/en/docs/http/load_balancing.html)
- [Refer Nginx Documentation here](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/)

Edit the NGINX configuration file (e.g., `/etc/nginx/nginx.conf`):

```nginx
worker_processes auto;

events {
    worker_connections 1024;
}

http {
    upstream backend_servers {
        server 172.31.22.151;
        server 172.31.22.152;
    }

    server {
        listen 80;

        location / {
            proxy_pass http://backend_servers;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
    }
}
```

#### Explanation:

1ï¸âƒ£ **worker_processes auto;**
ðŸ‘‰ Decides how many workers (helpers) Nginx should use. More workers = better handling of traffic.

2ï¸âƒ£ **events { worker_connections 1024; }**
ðŸ‘‰ Each worker can handle up to 1,024 visitors at the same time.

3ï¸âƒ£ **http {} (Main web settings)**

- **upstream backend_servers {}**
  - Defines a group of two servers (`172.31.22.152` & `172.31.17.27`).
  - Nginx will send traffic to them one by one (load balancing).

- **server { listen 80; }**
  - Listens for website visitors on port 80 (default web traffic port).

- **location / { proxy_pass http://backend_servers; }**
  - When someone visits your site, Nginx forwards the request to one of the backend servers.

- **proxy_set_header ...**
  - These lines help keep track of where the request came from.

#### Imagine This Scenario:
You have a call center where customers call a single phone number, but you have two customer service agents answering the calls.

- **Nginx is the receptionist ðŸ“ž**
- Customers (website visitors) call the main number (visit your website).
- Two customer service agents (backend servers) answer the calls.
- The receptionist (Nginx) forwards each call to one of the agents to balance the workload.

### 3. Check the nginx configuration
```bash
sudo nginx -t
```

### 4. Restart NGINX
```bash
sudo systemctl restart nginx
```

### 5. How It Works
- When a client accesses the load balancerâ€™s IP, NGINX will distribute the requests between `172.31.22.151` and `172.31.22.152`.
- If one server fails, traffic will continue to the remaining server.

This setup ensures availability and scalability.



---
---
---

# AWS Load Balancers

## Types of Load Balancers in AWS
AWS provides different types of load balancers to distribute incoming traffic across multiple targets. The main types are:

### 1. **Application Load Balancer (ALB)**
   - Operates at the **Layer 7 (Application Layer)** of the OSI model.
   - Supports **HTTP, HTTPS** protocols.
   - Best suited for applications that require **intelligent routing** based on request attributes like URL paths, host headers, etc.
   - Example Use Case: Routing requests to different microservices based on the path (e.g., `/api` requests go to one target group and `/user` requests go to another).

### 2. **Network Load Balancer (NLB)**
   - Operates at **Layer 4 (Transport Layer)**.
   - Supports **TCP, UDP, TLS** protocols.
   - Provides **low latency and high performance**, handling millions of requests per second.
   - Example Use Case: Load balancing TCP traffic for a real-time financial application.

### 3. **Gateway Load Balancer (GWLB)**
   - Works at **Layer 3 (Network Layer)**.
   - Primarily used for **inspecting traffic** and integrating with third-party security appliances.
   - Example Use Case: Distributing traffic to intrusion detection/prevention systems (IDS/IPS).

### 4. **Classic Load Balancer (CLB) [Legacy]**
   - Operates at **both Layer 4 (Transport) and Layer 7 (Application)**.
   - Supports **HTTP, HTTPS, TCP, and SSL**.
   - AWS recommends using ALB or NLB instead.
   - Example Use Case: Basic load balancing without advanced routing capabilities.

## Target Groups in AWS Load Balancer
A **target group** is a logical grouping of actual backend servers (EC2 instances, containers, Lambda functions, or IPs) behind a load balancer.
- When a request arrives, the load balancer forwards it to one of the targets in the associated target group.
- Each target group has its own **health check settings**.
- Target groups allow **fine-grained routing**, enabling different paths to be served by different sets of instances.

## Load Balancer Serving Requests in Private Subnets
A Load Balancer can serve requests in both **public (internet-facing)** and **private (internal-facing)** environments.

### **Internet-Facing Load Balancer**
- This type of load balancer has a **public IP address**.
- It routes incoming traffic from the internet to instances in public or private subnets.
- Example: A website with an ALB that serves users over the internet using an ACM SSL certificate.

### **Internal Load Balancer**
- This load balancer does **not have a public IP** and is accessible only within the VPC.
- It is used for **intranet applications** where traffic must remain private.
- Example: A company's HR portal that is only accessible within the corporate network.

## Listeners and Their Use Case
A **listener** is a process that checks for incoming connection requests using a specified **protocol and port**.
- Every load balancer **must have at least one listener** to accept traffic.
- A listener forwards requests to target groups based on defined **rules**.
- Example: An ALB with a listener on port 443 (HTTPS) that routes traffic to backend web servers.

## Why Load Balancers Perform Health Checks
Health checks ensure that traffic is sent only to **healthy instances**. The load balancer continuously monitors the targets and determines their status.

### **How Health Checks Work**
- The load balancer sends requests (health check probes) to each target at **regular intervals**.
- If a target **fails** the health check multiple times, it is marked as **unhealthy**, and traffic is no longer sent to it.
- If the instance becomes healthy again, it is re-added to the active rotation.

### **Health Check Parameters**
1. **Protocol**: The protocol used for health checks (e.g., HTTP, TCP, HTTPS).
2. **Port**: The port on which the health check is performed.
3. **Healthy Threshold**: Number of consecutive successful health checks before marking an instance as healthy.
4. **Unhealthy Threshold**: Number of consecutive failed health checks before marking an instance as unhealthy.
5. **Timeout**: The time to wait for a response from a target before considering the health check failed.
6. **Interval**: The time between two consecutive health check attempts.

### **Example Health Check Configuration**
- **Protocol**: HTTP
- **Port**: 80
- **Healthy Threshold**: 3
- **Unhealthy Threshold**: 2
- **Timeout**: 5 seconds
- **Interval**: 30 seconds

### **What Happens When a Health Check Fails?**
- The load balancer stops sending traffic to the unhealthy instance.
- If all instances in a target group fail, requests may **fail completely** unless a backup target group is configured.
- Once the instance recovers and passes the health check, it is **reintroduced** into the load balancing pool.

## Conclusion
AWS Load Balancers are essential for distributing traffic efficiently across multiple backend servers. Understanding key concepts such as **load balancer types, target groups, listeners, health checks, and private/public load balancing** helps in designing robust and scalable applications in AWS.

---
---
---

# Configuring AWS Network Load Balancer (NLB) to Route Traffic to Multiple Targets

## Scenario 1: Routing Traffic to Two EC2 Instances
We have two EC2 instances running the same application:
- **Instance 1**: `172.55.55.51:80`
- **Instance 2**: `172.55.55.52:80`

To set up a Network Load Balancer (NLB) to distribute traffic between these two instances, follow these steps:

### **Step 1: Create a Network Load Balancer**
1. Navigate to the **AWS Management Console**.
2. Go to the **EC2 Dashboard** > **Load Balancers**.
3. Click **Create Load Balancer** and choose **Network Load Balancer**.
4. Set the **scheme**:
   - Choose **Internet-facing** if users access it from the internet.
   - Choose **Internal** if it's only accessible within a VPC.
5. Select a **VPC** and at least **two availability zones** for high availability.
6. Configure the **listener**:
   - Protocol: **TCP**
   - Port: **80**

### **Step 2: Create a Target Group**
1. Navigate to **Target Groups**.
2. Click **Create Target Group**.
3. Select **Target Type** as **Instances**.
4. Set Protocol: **TCP**, Port: **80**.
5. Select the appropriate **VPC**.
6. Register the targets:
   - Add `172.55.55.51` and `172.55.55.52` to the target group.
7. Click **Create Target Group**.

### **Step 3: Attach the Target Group to the NLB**
1. Go back to the **Network Load Balancer** configuration.
2. Under **Listeners**, choose the previously created **target group**.
3. Click **Create Load Balancer**.

Now, any request coming to the NLB's DNS will be distributed between `172.55.55.51:80` and `172.55.55.52:80`.

---

## Scenario 2: Adding Another Application on a Different Port
We have another instance running on:
- **Instance 3**: `172.55.55.53:8080`

To attach it to the **same NLB**, follow these steps:

1. Create another **Target Group** for port `8080`.
2. Register **`172.55.55.53`** in this target group.
3. Add a new **Listener** to the NLB:
   - Protocol: **TCP**
   - Port: **8080**
   - Forward to the **new target group**.

Now, the NLB will handle traffic for:
- Port **80**: Routes to `172.55.55.51` and `172.55.55.52`
- Port **8080**: Routes to `172.55.55.53`

---

## Scenario 3: Routing Traffic Based on Application Context
We have:
- `172.55.55.51:80/java-application-1`
- `172.55.55.52:80/java-application-1`
- `172.55.55.53:8080/java-application-2`

### **Can We Achieve This Using NLB?**
No, **NLB cannot route traffic based on paths (URL paths like `/java-application-1`)** because it operates at **Layer 4 (TCP/UDP level)**.

### **Achieving This Using Two NLBs**
If we create:
- **NLB 1**: Handles traffic for `172.55.55.51:80` and `172.55.55.52:80`.
- **NLB 2**: Handles traffic for `172.55.55.53:8080`.

This will work because each NLB only considers **IP and port**, not URL paths.

### **Why One NLB Cannot Achieve This?**
A **single NLB cannot differentiate between application contexts (`/java-application-1` or `/java-application-2`)** because it only routes based on **IP and port**.

### **Solution Using an Application Load Balancer (ALB)**
If routing based on URL paths is required, an **Application Load Balancer (ALB)** should be used instead of NLB:
1. Create an **ALB**.
2. Set up **Listeners** on **port 80** and **port 8080**.
3. Create **Target Groups**:
   - One for `172.55.55.51` and `172.55.55.52` for **/java-application-1**.
   - One for `172.55.55.53` for **/java-application-2**.
4. Configure **Listener Rules** to forward requests based on URL paths:
   - Requests to `/java-application-1` â†’ Forward to the first target group.
   - Requests to `/java-application-2` â†’ Forward to the second target group.

This ensures requests are routed correctly based on the application context.

---

## Additional Considerations for NLB
### **Cross-Zone Load Balancing**
- By default, NLB distributes traffic to targets within the same **Availability Zone**.
- Enabling **Cross-Zone Load Balancing** ensures traffic is evenly distributed across all targets, regardless of the AZ.
- This is useful for applications that need **better load distribution** across multiple zones.

### **Health Checks**
- Ensure **health checks** are configured on each **Target Group**.
- If an instance becomes unhealthy, NLB stops routing traffic to it until it recovers.

### **IP-Based Targeting**
- Instead of targeting EC2 instances directly, you can register **IP addresses**.
- Useful for **ECS services** or targets outside the AWS environment.

### **Private Link Support**
- NLB supports **AWS PrivateLink**, allowing secure access to applications without exposing them to the internet.

---

## **Conclusion**
- **NLB is best for load balancing at the transport layer (Layer 4) and does not support routing based on URL paths.**
- **For routing based on URL paths, use an Application Load Balancer (ALB) instead.**
- **If different applications run on different ports, multiple NLBs can be used to handle them separately.**
- **Cross-zone load balancing, health checks, and private link support can optimize performance and security.**

---
---
---
