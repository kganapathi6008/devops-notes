# Kubernetes Volumes & Stateful Applications

---

## üß† **What Does "Stateless" Mean in Deployments?**

A **stateless application** doesn't maintain any information or session state about a user or process between different requests. Every request is treated as completely independent.

In Kubernetes, **Deployments** are ideal for stateless apps because:
- They can be **scaled easily** (add/remove pods as needed).
- Pods can be **terminated/restarted/rescheduled** without worrying about lost data.
- No session or file system state is stored inside the pod.

üì¶ Example of stateless apps:
- Frontend web apps (React, Angular)
- REST APIs (e.g., Spring Boot, Flask APIs)
- Load balancers or caching layers like NGINX or Redis (in ephemeral mode)

### ‚ö†Ô∏è What if You Don‚Äôt Use Volumes?
If a container writes data to its local filesystem and no volume is defined:
- That data is stored in the **container‚Äôs writable layer**, which is **ephemeral**.
- If the pod is **deleted, restarted, or rescheduled**, **all data is lost**.

Example:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: stateless-pod
spec:
  containers:
  - name: nginx
    image: nginx
    command: ["/bin/sh", "-c"]
    args:
      - echo "Some data" > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'
```
‚û°Ô∏è This data will be lost when the pod terminates, because there's no volume defined.

---

## ü§î **Why Do Some Applications Need to Be Stateful?**

A **stateful application** maintains data across requests or pod restarts. This data can be:
- User session info
- Uploaded files
- Database entries
- Cache states

üìå **When do you need stateful apps?**
- When the app stores data that **must not be lost** across pod restarts.
- When each pod needs a **stable identity or hostname** (e.g., databases like MySQL, MongoDB).

üì¶ Examples of stateful apps:
- Databases (MySQL, PostgreSQL, MongoDB)
- Message queues (Kafka, RabbitMQ)
- Storage services (MinIO, ElasticSearch)

---

## üìÅ **Why Volumes Came Into the Picture**

In Kubernetes, when a Pod dies or is deleted:
- The **ephemeral container storage is lost** (like `/tmp`, `/usr/share/nginx/html`, etc.)
- All data written inside the container is **gone** unless it's stored externally.

To fix this, **Volumes** were introduced to **persist data** beyond the pod lifecycle.

> Volumes are **external to the container** but attached to the pod, ensuring data survives even if the container crashes or restarts.

---

## üîß **Types of Volumes in Kubernetes**

### 1. **emptyDir**
- Created when a pod is assigned to a node.
- Lives as long as the pod lives.
- Data is deleted when pod is deleted.
- Useful for temporary scratch space **shared between containers** in the same pod.

‚ö†Ô∏è **Important**:
- If you don‚Äôt define any volume (not even emptyDir), data is written inside the container layer ‚Äî which is **non-persistent**.
- Even if you define an `emptyDir`, the data is still **non-persistent**, as it only lives as long as the pod exists.
- When the pod is deleted, restarted, or moved to another node, the `emptyDir` and its contents are also deleted.
- It's mainly useful for temporary storage or data sharing between containers in the same pod ‚Äî not for true persistence across pod lifecycles.

### 2. **hostPath**
- Mounts a file or directory from the host node‚Äôs filesystem into the pod.
- Not portable or suitable for production.

### 3. **PersistentVolume (PV) + PersistentVolumeClaim (PVC)**
- Designed for **stateful apps**.
- Decouples storage from pods.
- Supports dynamic provisioning using StorageClasses.

### 4. **configMap & secret volumes**
- Used to inject config and secret data into pods as files.

### 5. **nfs, awsElasticBlockStore, gcePersistentDisk, etc.**
- Cloud-provider-specific volumes for persistent storage.

---

## üèë **Summary**
- **Stateless apps** don‚Äôt store data; they are easily scalable and managed by Deployments.
- **Stateful apps** require data persistence, identity, and order ‚Äî managed using StatefulSets + Volumes.
- **Volumes** help in persisting data across pod lifecycles.
- **PersistentVolume + PersistentVolumeClaim** is the standard way to achieve long-term storage in Kubernetes.
- Use **Deployments** for stateless services and **StatefulSets + PVCs** for stateful workloads.

---
---
---

# Volume Types :- MongoDB Volume Persistence Examples in Kubernetes (Using Deployment)
This document shows how **different Kubernetes volumes** affect MongoDB data persistence. For each volume type, we'll:

- Deploy a MongoDB **deployment** with that volume type
- Insert data inside the MongoDB pod
- Delete the pod
- Recreate the pod
- Verify whether the data is still present (i.e., check persistence)
- Each deployment will also include **username/password authentication** set using environment variables

---

## üìÅ 1. emptyDir Volume

### Explanation:
- `emptyDir` is a temporary volume that exists as long as the pod exists.
- It is useful for sharing data between containers in the same pod.
- Once the pod is deleted, the data is **lost**.

### Deployment YAML:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-emptydir
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-emptydir
  template:
    metadata:
      labels:
        app: mongo-emptydir
    spec:
      containers:
      - name: mongo
        image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: secret123
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        emptyDir: {}
```

### Steps to Test:
```bash
kubectl apply -f mongo-emptydir.yaml
kubectl get pods -l app=mongo-emptydir
kubectl exec -it <pod-name> -- mongosh -u admin -p secret123 --authenticationDatabase admin
kubectl exec -it <pod-name> ls /data/db
```

Inside the Mongo shell:
```js
// Switch to the "test" database (creates it if it doesn't exist)
test> db = db.getSiblingDB("test")

// Insert multiple user documents into a "users" collection
test> db.users.insertMany([
  {name: "Ganapathi", email: "ganapathi@example.com", mob: "9876543210", area: "Hyderabad"},
  {name: "John", email: "john@example.com", mob: "9999999999", area: "Bangalore"},
  {name: "Jane", email: "jane@example.com", mob: "8888888888", area: "Chennai"}
])


// Retrieve and display all documents from the "users" collection in a readable format
test> db.users.find().pretty()
// see the output like this
[
  {
    _id: ObjectId('67f39b040e8dc99cba6b140b'),
    name: 'Ganapathi',
    email: 'ganapathi@example.com',
    mob: '9876543210',
    area: 'Hyderabad'
  },
  {
    _id: ObjectId('67f39b040e8dc99cba6b140c'),
    name: 'John',
    email: 'john@example.com',
    mob: '9999999999',
    area: 'Bangalore'
  },
  {
    _id: ObjectId('67f39b040e8dc99cba6b140d'),
    name: 'Jane',
    email: 'jane@example.com',
    mob: '8888888888',
    area: 'Chennai'
  }
]
```
Then delete the pod:
```bash
kubectl delete pod -l app=mongo-emptydir
```
Recheck the data after the new pod is recreated automatically:
```bash
kubectl get pods -l app=mongo-emptydir
kubectl exec -it <new-pod-name> -- mongosh -u admin -p secret123 --authenticationDatabase admin
test> db.getSiblingDB("test").users.find().pretty()
```
‚û°Ô∏è **Result:** Data is **lost** because `emptyDir` is deleted with the pod.

---

## üìÅ 2. hostPath Volume

### Explanation:
- `hostPath` mounts a directory from the host node into the pod.
- The data persists as long as the same node is used.
- Not suitable for production because it couples the pod to a specific node.

### Deployment YAML:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-hostpath
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-hostpath
  template:
    metadata:
      labels:
        app: mongo-hostpath
    spec:
      containers:
      - name: mongo
        image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: secret123
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        hostPath:
          path: /tmp/mongo-data             ## This will create `/tmp/mongo-data` on your node.
          type: DirectoryOrCreate
```
‚û°Ô∏è **Result:** Data is **persisted** as it‚Äôs stored on the node filesystem.

Run the following command to know on which node the pod is allocated:
```bash
kubectl get pods -o wide    ## you can see the node details
```

### ‚ùó Problem:
- If pod is rescheduled on another node, data will be **lost**.
- Since the `hostPath` mounts local storage on a specific node, if the pod is deleted and recreated on another node, it won't find the original data.


### ‚úÖ Solution: Use `nodeAffinity`
We can force the pod to always schedule on the **same node** using `nodeAffinity`, which ensures data persistence with `hostPath`.

### üîç How to get the node hostname:
Run the following command to get the node's hostname:
```bash
kubectl get nodes --show-labels
```
Look for the label:
```
kubernetes.io/hostname=ip-192-168-102-53.ap-south-2.compute.internal
```
This value will be used in the `nodeAffinity` section.

### Updated Deployment YAML with `hostPath` + `nodeAffinity`
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-hostpath
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-hostpath
  template:
    metadata:
      labels:
        app: mongo-hostpath
    spec:
      affinity:  # Defines rules that influence where the pod is scheduled
        nodeAffinity:  # Specifies node-level affinity (which node to run the pod on)
          requiredDuringSchedulingIgnoredDuringExecution:  # This rule is required at scheduling time, ignored later during pod execution
            nodeSelectorTerms:  # A list of node selection criteria (OR logic between items)
            - matchExpressions:  # A list of conditions (AND logic between conditions)
              - key: kubernetes.io/hostname  # Built-in node label key representing node name
                operator: In  # Means the value must match one in the list below
                values:
                - ip-192-168-102-53.ap-south-2.compute.internal  # The exact node name to schedule the pod on
      containers:
      - name: mongo
        image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: secret123
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        hostPath:
          path: /tmp/mongo-data
          type: DirectoryOrCreate
```

‚û°Ô∏è **Result:** Now even if the pod is deleted and recreated, it will always be scheduled on the same node, and the data will persist **as long as that node exists**.

### ‚ö° Important:
If the node itself is **deleted or becomes unavailable**, the `hostPath` volume and its data are also lost permanently. Since `hostPath` relies on the local storage of a specific node, there is **no high availability or portability** of the data. This is a major drawback for production environments.

---

## üìÅ 3. PersistentVolume + PersistentVolumeClaim (Static Provisioning)

### Explanation:
- This is the standard way to persist data in Kubernetes.
- The EBS volume is **pre-created manually** (static provisioning) and **bound** to a PersistentVolumeClaim (PVC).
- The PVC can be reused even after the pod is deleted or recreated.

> üßê **Important:**
> If you're running your cluster on **AWS**, you must ensure that the **Amazon EBS CSI Driver** is installed and configured.
> Kubernetes uses this driver to interact with Amazon EBS volumes ‚Äî handling volume attachment, mounting, and lifecycle events.
> Without the CSI driver, Kubernetes cannot mount or manage EBS volumes for your pods, and this static PV + PVC setup will not work.

### EBS Volumes are AZ-Specific:
Amazon EBS volumes are tied to a specific **Availability Zone (AZ)**. You cannot attach a volume in `ap-south-2a` to a node in `ap-south-2b`.

Therefore, when using static provisioning with EBS, it's critical to:
- Create the EBS volume in the **same AZ** as the Kubernetes node that will mount it.
- Use `nodeAffinity` in the PersistentVolume to ensure the volume only gets attached to nodes in the correct AZ.

### Steps to Use Static Provisioning:
1. Create a PersistentVolume (PV) with node affinity
2. Create a PersistentVolumeClaim (PVC) that matches the PV
3. Mount the PVC inside the MongoDB pod

### üîç Verifying Node AZ Labels:
To check what zones your nodes are in:
```bash
kubectl get nodes --show-labels
```
Look for a label like:
```
topology.kubernetes.io/zone=ap-south-2a
```
If the label is missing or incorrect:
```bash
kubectl label node <node-name> topology.kubernetes.io/zone=ap-south-2a
```

### üîß How to Create an EBS Volume and Get Its ID (AWS):
1. Go to the **EC2 Dashboard** in AWS Console.
2. In the left sidebar, click **Volumes** under "Elastic Block Store".
3. Click **Create Volume**.
4. Choose the following options:
   - **Volume Type:** gp2 (or gp3)
   - **Size:** e.g., 1 GiB
   - **Availability Zone:** Must match your Kubernetes worker node AZ (e.g., `ap-south-2a`)
5. Click **Create Volume**.
6. After creation, note the **Volume ID** (e.g., `vol-047f6bbbe478b6075`).

‚û°Ô∏è This method ensures that the volume already exists before the pod uses it, making it a static provisioning setup.

### üì¶ PersistentVolume Example (with Node Affinity):
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongo-static-pv
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  csi:
    driver: ebs.csi.aws.com
    volumeHandle: vol-047f6bbbe478b6075  # Replace with your EBS Volume ID
    fsType: ext4
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - ap-south-2a
  storageClassName: ""
```

### üìÑ PersistentVolumeClaim:
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongo-static-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: ""
```
‚úÖ Kubernetes will automatically bind this PVC to the matching PV based on access mode, capacity, and empty storage class.

### üß± Deployment YAML:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-static
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-static
  template:
    metadata:
      labels:
        app: mongo-static
    spec:
      containers:
      - name: mongo
        image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: secret123
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        persistentVolumeClaim:
          claimName: mongo-static-pvc
```

‚û°Ô∏è **Result:**
Data is **persisted** even after pod deletion. As long as the EBS CSI Driver is installed, the volume is correctly bound, and the node affinity matches the node's AZ, your MongoDB data will remain intact.

### ‚ö†Ô∏è Pod Scheduling Behavior:
If no suitable node exists in the AZ specified in the PV's nodeAffinity, the pod will not be scheduled.

Example error:
```text
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  22s   default-scheduler  0/1 nodes are available: 1 node(s) had volume node affinity conflict. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
```

üîç What this means:
Kubernetes tried to schedule the pod, but none of the available nodes matched the volume's `nodeAffinity`. This usually means the pod cannot be scheduled because it can only use the volume on nodes in a specific AZ (e.g., `ap-south-2a`).

-  Ensure that your node pool includes at least one node in the specified AZ.
- Double-check volume AZ and node labels.
- Use `kubectl describe pod <pod-name>` to troubleshoot scheduling issues.

### üí° Tip:
- A PV can only be bound to **one PVC**.
- Even if the PVC requests less than the PV size, the full PV is **reserved**.
- Remaining space in the PV **cannot be reused** by other PVCs.
- To avoid waste, create smaller PVs that match your application's actual storage needs.


### üõ†Ô∏è Troubleshooting: PV Stuck in "Terminating" State
If a PVC is deleted before the PV and the `persistentVolumeReclaimPolicy` is set to `Retain`, the PV might get stuck in a `Terminating` state. This is due to Kubernetes waiting to perform clean-up defined by the `finalizers`.

#### üßπ Solution: Remove Finalizers from PV
To force delete the PV, run:
```bash
kubectl patch pv mongo-static-pv -p '{"metadata":{"finalizers":null}}' --type=merge
```
‚úÖ This will remove the finalizer and allow the PV to be deleted immediately.

üìå Use this only when you're sure the volume is no longer needed and safe to remove manually.

---

## üìÅ 4. PersistentVolumeClaim with Dynamic Volume Provisioning

### üöÄ What is Dynamic Provisioning?
- Dynamic provisioning allows Kubernetes to automatically create a PersistentVolume (PV) when a PersistentVolumeClaim (PVC) is made.
- No need to manually pre-create volumes (like in static provisioning).
- Great for cloud environments (AWS, GCP, Azure) where storage APIs are available.

### ‚öôÔ∏è Requirements:
- A working **StorageClass**.
- A **CSI Driver** (like AWS EBS CSI driver) must be installed.

> üß† **Note:** If you don‚Äôt define a `storageClassName` in your PVC, Kubernetes may use the default StorageClass.

---

### üì¶ StorageClass Example (for AWS EBS):
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: thisismynewsc
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
```

### üìÑ PersistentVolumeClaim:
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongo-dynamic-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: thisismynewsc
```

‚û°Ô∏è Kubernetes will use the `thisismynewsc` StorageClass to provision a 1Gi volume. That means `pv - (ebs volume)` is going to create automatically whenever we request it.

---

### üß± Deployment Example:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-dynamic
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-dynamic
  template:
    metadata:
      labels:
        app: mongo-dynamic
    spec:
      containers:
      - name: mongo
        image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: secret123
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        persistentVolumeClaim:
          claimName: mongo-dynamic-pvc
```

---

### ‚úÖ Result:
- Kubernetes provisions an EBS volume when PVC is created.
- Volume is automatically attached to the pod.
- When the PVC is deleted, the volume can also be deleted (depending on `reclaimPolicy`).

---

### üìò Tip:
- Dynamic provisioning saves time and reduces errors.
- Use it when you don‚Äôt want to manage storage manually.
- Works great with StatefulSets too.

---

### üßæ About `reclaimPolicy`
- The `reclaimPolicy` is specified inside the **StorageClass**.
- It controls what happens to the volume **after the PVC is deleted**.

#### Types of `reclaimPolicy`:
- `Delete` - Deletes the underlying storage automatically.
- `Retain` - Keeps the volume; you must delete it manually.
- `Recycle` - Deprecated and not supported in most environments.

> ‚úÖ **Recommended**: Use `Delete` for dynamic provisioning in most cloud-native use cases.


---

### ‚õìÔ∏è About `volumeBindingMode`
- `volumeBindingMode` defines **when** the PersistentVolume is bound to a PersistentVolumeClaim.

#### Types:
- `Immediate` (default):
  - The volume is provisioned as soon as the PVC is created.
  - May lead to pod scheduling problems if volume and pod are in different zones.

- `WaitForFirstConsumer`:
  - The volume is provisioned **only when a pod using the PVC is scheduled**.
  - Ensures that the volume is created in the **same Availability Zone** as the pod.

> üìå **Best Practice**: Use `WaitForFirstConsumer` in multi-AZ clusters to avoid volume and pod AZ mismatch issues.

---

### üìö About `accessModes`
The `accessModes` field defines **how many nodes can mount the volume at the same time**.

#### Types:
- `ReadWriteOnce` (RWO):
  - Can be mounted as read-write by **a single node**.
  - Most commonly used mode (especially for EBS).

- `ReadOnlyMany` (ROX):
  - Can be mounted as read-only by **many nodes** simultaneously.

- `ReadWriteMany` (RWX):
  - Can be mounted as read-write by **many nodes**.
  - Requires a networked filesystem (like NFS, or EFS on AWS).

> ‚ÑπÔ∏è For AWS EBS, only `ReadWriteOnce` is supported.


---

### üìå Common Troubleshooting:
- **PVC Pending**: Check if the StorageClass exists and the CSI driver is installed.
- **Pod not starting**: Check `kubectl describe pod <name>` for volume-related events.
- **Volume not created**: Check cloud permissions and controller logs.

> üì¶ Dynamic provisioning is cloud-native and DevOps-friendly!

---

### üêõ Multi-Attach Error in EBS Volumes (Kubernetes)
- AWS EBS volumes support only **ReadWriteOnce (RWO)** mode.
- That means an EBS volume **can only be attached to one node at a time**.

#### ‚ùå Scenario:
Suppose your pod is running with a dynamically provisioned volume, and the **node gets deleted** or crashes.
- Kubernetes will try to recreate the pod on a different node.
- However, the original pod may still be in `Terminating` state, and the volume is still considered attached to the old node.
- You'll see an error like:

```
Warning  FailedAttachVolume  attachdetach-controller  Multi-Attach error for volume "<pvc-id>": Volume is already used by pod(s) <pod-name>
```

#### üí° Solution:
- Kubernetes can‚Äôt detach and re-attach the volume until the old pod is fully terminated.
- To avoid this:
  - Use **StatefulSet** instead of Deployment for volume-bound workloads like databases.
  - StatefulSets have stable pod identity and handle volume lifecycle better.
  - Also consider setting a `terminationGracePeriodSeconds` to allow graceful pod shutdown.

---
### üõ†Ô∏è Troubleshooting Common Issues

| Problem | Possible Cause |
|--------|------------------|
| PVC stuck in `Pending` | No available StorageClass or provisioning failed |
| Pod stuck in `ContainerCreating` | Volume not yet attached or available |
| IAM access denied | Missing EC2 or EBS permissions |
| Multi-AZ errors | `volumeBindingMode` not set to `WaitForFirstConsumer` |
| Multi-Attach error | EBS volumes are RWO - cannot attach to multiple nodes |
| Node not ready | Taints or missing CSI driver on node |
| PVC bound but pod crashlooping | App-level error or volume permission issues |
| Node has volume node affinity conflict | Pod scheduled to different AZ from volume |

---

## ‚úÖ Summary Table
| Volume Type                  | Data Persisted After Pod Deletion? | Notes |
|-----------------------------|-------------------------------------|-------|
| emptyDir                    | ‚ùå No                              | Temporary for pod lifetime only |
| hostPath                    | ‚úÖ Yes (on same node)              | Not portable; data lost on different node |
| PVC (Static)                | ‚úÖ Yes                             | Manual setup of PV or default provisioner |
| PVC (Dynamic + StorageClass)| ‚úÖ Yes                             | Automatic volume management |

---
### üß† Can I Scale a Deployment That Uses a Single PVC?

- ‚ùå No, scaling a Deployment with `replicas > 1` using a single PVC backed by EBS won't work.
- AWS EBS volumes support `ReadWriteOnce` ‚Äî meaning they can be attached to only **one pod on one node**.
- If you try scaling such a Deployment, the additional pods will fail to start due to **Multi-Attach errors**.

---

### ‚úÖ Recommended Solution

- Use a **StatefulSet** with `volumeClaimTemplates`.
- Each replica gets a **unique PVC and EBS volume**.
- Ideal for databases like MongoDB that require per-instance storage.

---
---
---
