# ü§î Why Use StatefulSet if Deployment + PVC Can Persist Data?
- https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
- https://www.eksworkshop.com/docs/fundamentals/storage/ebs/statefulset-with-ebs

While **Deployments with PVCs** can persist data, they are still designed for **stateless** workloads.

Problems with Deployment for stateful apps:
- Pod names are **not stable**. Each new pod gets a new random name (like `app-5c7f9f8c7d-x9k7t`).
- PVCs are not automatically reused with the same pod. (Increasing replicas in your current Deployment will cause pods to conflict over the single PVC, likely resulting in failed pods.)
- No **ordered startup or shutdown**.
- No stable **network identity** or hostname.

### ‚úÖ What StatefulSet Provides:
- **Stable, sticky identity** for each pod (`pod-0`, `pod-1`, etc.)
- **Consistent network hostname** (e.g., `mongo-0.mongo-headless.default.svc.cluster.local`)
- **Stable volume claim per pod** (e.g., `data-mongo-0`, `data-mongo-1`)
- **Ordered** deployment, scaling, and termination.

> So, use **StatefulSets** when your app needs stable identity, hostname, or storage across restarts and scaling. (Use a StatefulSet with volumeClaimTemplates so each pod gets its own PVC and volume)

üì¶ Examples:
- MongoDB replicaset pods
- MySQL primary/replica setup
- Kafka brokers

#### Comparison Table:
| Feature | Deployment + PVC | StatefulSet |
|--------|------------------|-------------|
| Data Persistence | ‚úÖ Yes | ‚úÖ Yes |
| Stable Network Identity | ‚ùå No | ‚úÖ Yes (`pod-0`, `pod-1`) |
| Stable Storage | ‚ö†Ô∏è Shared PVC | ‚úÖ One PVC per pod |
| Ordered Deployment | ‚ùå No | ‚úÖ Yes |
| Use case | Simple apps | DBs, Kafka, Zookeeper |

---

## üìÅ StatefulSet + MongoDB ReplicaSet with PVCs

This setup deploys a MongoDB ReplicaSet using a Kubernetes StatefulSet, persistent volumes (via a custom `StorageClass`), and a headless service for stable network identity. This enables high availability and data persistence.

---

### üì¶ mongo-storageclass.yaml
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: thisismynewsc
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  fsType: ext4
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
```

- **Purpose**: Creates a StorageClass named `thisismynewsc` using the AWS EBS CSI driver.
- **WaitForFirstConsumer**: Ensures volumes are provisioned only after pod scheduling to avoid AZ mismatch.
- **Delete** policy: Delete the volume data after PVC deletion for safety.

---

### üîå mongo-headless-service.yaml
```yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo-statefulset-service
spec:
  ports:
    - port: 27017
      targetPort: 27017
  clusterIP: None
  selector:
    app: mongo-statefulset
```

- **Purpose**: Provides stable network identities for each MongoDB pod via DNS (`mongo-statefulset-0.mongo-statefulset-service`).
- **clusterIP: None**: Makes it headless to directly expose pod DNS.

---

### üß± mongo-statefulset.yaml
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo-statefulset
spec:
  serviceName: "mongo-statefulset-service"
  replicas: 3
  selector:
    matchLabels:
      app: mongo-statefulset
      environment: test
  template:
    metadata:
      labels:
        app: mongo-statefulset
        environment: test
    spec:
      terminationGracePeriodSeconds: 10
      containers:
        - name: mongo
          image: mongo
          command:
            - mongod
            - "--replSet"
            - rs0
            - "--bind_ip_all"
            - "--port"
            - "27017"
          ports:
            - containerPort: 27017
          volumeMounts:
            - name: mongo-statefulset-storage
              mountPath: /data/db
  volumeClaimTemplates:
    - metadata:
        name: mongo-statefulset-storage
      spec:
        storageClassName: "thisismynewsc"
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 5Gi
```

- **command**: Ensures MongoDB starts in replica set mode.
- **StatefulSet**: Ensures stable pod identity and storage (e.g., `mongo-statefulset-0`).
- **PVC Template**: Requests 5Gi persistent volume for each pod using the `thisismynewsc`.


---

### üîÅ ReplicaSet Initialization Steps

To form a MongoDB ReplicaSet inside a Kubernetes StatefulSet, we need to initialize and configure the set manually from one of the pods. Here's why and how:

1. **Exec into the first pod (`mongo-statefulset-0`)**:
   ```bash
   kubectl exec -it mongo-statefulset-0 -- bash
   ```
   - Since `mongo-statefulset-0` is the first pod (and always comes up first), it's ideal for initiating the replica set.

2. **Connect to the Mongo shell**:
   ```bash
   mongosh
   ```
   - This connects us to the MongoDB instance running on that pod.

3. **Initiate the replica set**:
   ```javascript
   rs.initiate({
     _id: "rs0",
     members: [
       { _id: 0, host: "mongo-statefulset-0.mongo-statefulset-service:27017" },
       { _id: 1, host: "mongo-statefulset-1.mongo-statefulset-service:27017" },
       { _id: 2, host: "mongo-statefulset-2.mongo-statefulset-service:27017" }
     ]
   })
   ```
   - This sets up the initial configuration of the ReplicaSet.
   - Uses the headless service `mongo-statefulset-service` to resolve pod names (`mongo-statefulset-0.mongo-statefulset-service`, etc.).
   - Ensures MongoDB knows all the members of the set.

4. **Check ReplicaSet status**:
   ```javascript
   rs.status()
   ```
   - Verifies that the ReplicaSet has been properly initialized.
   - You should see the state as `PRIMARY`, `SECONDARY`, etc., for the pods.

---
### üß† MongoDB ReplicaSet Behavior

- After running `rs.initiate()`, one pod becomes **Primary**, others become **Secondaries**.
- ‚úÖ Writes are only allowed on the Primary.
- ‚ùå By default, reads are not allowed on Secondaries ‚Äî but can be enabled.
- Use `rs.status()` to monitor the replica set.
- ReplicaSets ensure **high availability**, **failover**, and **replication** in a StatefulSet environment.

---
### StatefulSet and EBS Volume Multi-Attach Issue Explained

When using AWS EBS volumes with Kubernetes StatefulSets, it's important to understand how volume attachment and detachment behaves, especially during node failures or pod restarts.

### Scenario: Node Deleted, EBS Still Attached

When a node running a StatefulSet pod (e.g., `mongo-statefulset-0`) gets terminated (due to scaling down, an issue, or a node rotation), the following sequence can occur:

1. **EBS Volume Attachment Still Active:**
   - The EBS volume used by the StatefulSet pod remains in an **attached** state to the old (terminated) node.
   - This can be seen in the AWS console or CLI under EC2 ‚Üí Volumes ‚Üí Attached Instances.

2. **New Pod Scheduled on New Node:**
   - Kubernetes tries to reschedule the pod on a new node.
   - The pod attempts to attach the same persistent volume (PVC) to the new node.

3. **Multi-Attach Error Occurs:**
   - Since EBS volumes support only **single attachment** in `ReadWriteOnce` mode, the new node cannot attach the volume while it is still attached to the old (terminated) node.
   - This results in an event like:
     ```
     Multi-Attach error for volume "pvc-xxxxx" Volume is already exclusively attached to one node and can't be attached to another
     ```

4. **Automatic Resolution:**
   - After some time (usually in 10 minutes), AWS detects that the old node is gone and detaches the volume.
   - Kubernetes then retries the attach operation.
   - Once successful, the pod can start normally.

### Sample Events:
```shell
Events:
  Type     Reason                  Age                   From                     Message
  ----     ------                  ----                  ----                     -------
  Warning  FailedScheduling        8m14s (x9 over 9m9s)  default-scheduler        no nodes available to schedule pods
  Warning  FailedScheduling        8m4s                  default-scheduler        0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled               7m54s                 default-scheduler        Successfully assigned default/mongo-statefulset-0 to ip-172-31-19-201.eu-west-2.compute.internal
  Warning  FailedAttachVolume      7m53s                 attachdetach-controller  Multi-Attach error for volume "pvc-ee13fb13-6a7f-4d6f-a2c8-8776f9d1d53a" Volume is already exclusively attached to one node and can't be attached to another
  Normal   SuccessfulAttachVolume  3m4s                  attachdetach-controller  AttachVolume.Attach succeeded for volume "pvc-ee13fb13-6a7f-4d6f-a2c8-8776f9d1d53a"
  Normal   Pulling                 3m3s                  kubelet                  Pulling image "mongo:6.0"
  Normal   Pulled                  2m51s                 kubelet                  Successfully pulled image "mongo:6.0" in 11.489s (11.489s including waiting). Image size: 262354007 bytes.
  Normal   Created                 2m51s                 kubelet                  Created container: mongo
  Normal   Started                 2m51s                 kubelet                  Started container mongo
```

### Solutions and Best Practices:

#### üîç Find Where the Volume is Attached
You can use AWS CLI or Console to check the volume attachment:
```bash
aws ec2 describe-volumes --volume-ids <volume-id>
```
This shows which instance the volume is currently attached to.

#### üßπ Detach the Volume Manually
If the volume is stuck in an `in-use` state:
```bash
aws ec2 detach-volume --volume-id <volume-id>
```

#### üí£ Force Delete the Pod
If the pod is stuck in terminating state and holding on to the volume:
```bash
kubectl delete pod <pod-name> --grace-period=0 --force
```
This immediately kills the pod and allows the volume to be released.

---

### Summary
- EBS volumes take time to detach from terminated nodes.
- New pods can't attach the same volume until it's fully detached.
- Kubernetes resolves this automatically, but you can speed it up with manual intervention.
- Monitor events and logs for attach/detach issues.

These behaviors are normal but critical to understand when using **StatefulSets** and **EBS** with Kubernetes on AWS.

---

### ‚úÖ Benefits:
- Dedicated PVC per pod ensures safe failover
- ReplicaSet works with consistent pod hostnames
- Sidecar auto-handles replica member addition
- Best suited for production MongoDB clusters


---
---
---
### The above Mongo DB replica-set is not a complete setup. For Mongo DB complete replica set for the application perspective, go through the following links

- https://kubernetes.io/blog/2017/01/running-mongodb-on-kubernetes-with-statefulsets/
- https://github.com/thesandlord/mongo-k8s-sidecar
- https://medium.com/codelogicx/mongodb-replicaset-on-kubernetes-with-auto-failover-auto-scaling-d15901658136
- https://github.com/AnkanDas97/k8s-mongo-replicaset
- https://youtu.be/W-lJX3_uE5I