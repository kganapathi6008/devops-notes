# MongoDB Volume Persistence Examples in Kubernetes (Using Deployment)

This document shows how **different Kubernetes volumes** affect MongoDB data persistence. For each volume type, we'll:

- Deploy a MongoDB **deployment** with that volume type
- Insert data inside the MongoDB pod
- Delete the pod
- Recreate the pod
- Verify whether the data is still present (i.e., check persistence)
- Each deployment will also include **username/password authentication** set using environment variables

---

## üìÅ 1. emptyDir Volume

### Explanation:
- `emptyDir` is a temporary volume that exists as long as the pod exists.
- It is useful for sharing data between containers in the same pod.
- Once the pod is deleted, the data is **lost**.

### Deployment YAML:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-emptydir
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-emptydir
  template:
    metadata:
      labels:
        app: mongo-emptydir
    spec:
      containers:
      - name: mongo
        image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: secret123
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        emptyDir: {}
```

### Steps to Test:
```bash
kubectl apply -f mongo-emptydir.yaml
kubectl get pods -l app=mongo-emptydir
kubectl exec -it <pod-name> -- mongosh -u admin -p secret123 --authenticationDatabase admin
```
Inside the Mongo shell:
```js
// Switch to the "test" database (creates it if it doesn't exist)
test> db = db.getSiblingDB("test")

// Insert multiple user documents into a "users" collection
test> db.users.insertMany([
  {name: "Ganapathi", email: "ganapathi@example.com", mob: "9876543210", area: "Hyderabad"},
  {name: "John", email: "john@example.com", mob: "9999999999", area: "Bangalore"},
  {name: "Jane", email: "jane@example.com", mob: "8888888888", area: "Chennai"}
])


// Retrieve and display all documents from the "users" collection in a readable format
test> db.users.find().pretty()
// see the output like this
[
  {
    _id: ObjectId('67f39b040e8dc99cba6b140b'),
    name: 'Ganapathi',
    email: 'ganapathi@example.com',
    mob: '9876543210',
    area: 'Hyderabad'
  },
  {
    _id: ObjectId('67f39b040e8dc99cba6b140c'),
    name: 'John',
    email: 'john@example.com',
    mob: '9999999999',
    area: 'Bangalore'
  },
  {
    _id: ObjectId('67f39b040e8dc99cba6b140d'),
    name: 'Jane',
    email: 'jane@example.com',
    mob: '8888888888',
    area: 'Chennai'
  }
]
```
Then delete the pod:
```bash
kubectl delete pod -l app=mongo-emptydir
```
Recheck the data after the new pod is recreated automatically:
```bash
kubectl get pods -l app=mongo-emptydir
kubectl exec -it <new-pod-name> -- mongosh -u admin -p secret123 --authenticationDatabase admin
test> db.getSiblingDB("test").users.find().pretty()
```
‚û°Ô∏è **Result:** Data is **lost** because `emptyDir` is deleted with the pod.

---

## üìÅ 2. hostPath Volume

### Explanation:
- `hostPath` mounts a directory from the host node into the pod.
- The data persists as long as the same node is used.
- Not suitable for production because it couples the pod to a specific node.

### Deployment YAML:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-hostpath
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-hostpath
  template:
    metadata:
      labels:
        app: mongo-hostpath
    spec:
      containers:
      - name: mongo
        image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: secret123
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        hostPath:
          path: /tmp/mongo-data             ## This will create `/tmp/mongo-data` on your node.
          type: DirectoryOrCreate
```
‚û°Ô∏è **Result:** Data is **persisted** as it‚Äôs stored on the node filesystem.

Run the following command to know on which node the pod is allocated:
```bash
kubectl get pods -o wide    ## you can see the node details
```

### ‚ùó Problem:
- If pod is rescheduled on another node, data will be **lost**.
- Since the `hostPath` mounts local storage on a specific node, if the pod is deleted and recreated on another node, it won't find the original data.


### ‚úÖ Solution: Use `nodeAffinity`
We can force the pod to always schedule on the **same node** using `nodeAffinity`, which ensures data persistence with `hostPath`.

### üîç How to get the node hostname:
Run the following command to get the node's hostname:
```bash
kubectl get nodes --show-labels
```
Look for the label:
```
kubernetes.io/hostname=ip-192-168-102-53.ap-south-2.compute.internal
```
This value will be used in the `nodeAffinity` section.

### Updated Deployment YAML with `hostPath` + `nodeAffinity`
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-hostpath
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-hostpath
  template:
    metadata:
      labels:
        app: mongo-hostpath
    spec:
      affinity:  # Defines rules that influence where the pod is scheduled
        nodeAffinity:  # Specifies node-level affinity (which node to run the pod on)
          requiredDuringSchedulingIgnoredDuringExecution:  # This rule is required at scheduling time, ignored later during pod execution
            nodeSelectorTerms:  # A list of node selection criteria (OR logic between items)
            - matchExpressions:  # A list of conditions (AND logic between conditions)
              - key: kubernetes.io/hostname  # Built-in node label key representing node name
                operator: In  # Means the value must match one in the list below
                values:
                - ip-192-168-102-53.ap-south-2.compute.internal  # The exact node name to schedule the pod on
      containers:
      - name: mongo
        image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: secret123
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        hostPath:
          path: /tmp/mongo-data
          type: DirectoryOrCreate
```

‚û°Ô∏è **Result:** Now even if the pod is deleted and recreated, it will always be scheduled on the same node, and the data will persist **as long as that node exists**.

### ‚ö° Important:
If the node itself is **deleted or becomes unavailable**, the `hostPath` volume and its data are also lost permanently. Since `hostPath` relies on the local storage of a specific node, there is **no high availability or portability** of the data. This is a major drawback for production environments.

---

## üìÅ 3. PersistentVolume + PersistentVolumeClaim (Static Provisioning)

### Explanation:
- This is the standard way to persist data in Kubernetes.
- The EBS volume is **pre-created manually** (static provisioning) and **bound** to a PersistentVolumeClaim (PVC).
- The PVC can be reused even after the pod is deleted or recreated.

> üßê **Important:**
> If you're running your cluster on **AWS**, you must ensure that the **Amazon EBS CSI Driver** is installed and configured.
> Kubernetes uses this driver to interact with Amazon EBS volumes ‚Äî handling volume attachment, mounting, and lifecycle events.
> Without the CSI driver, Kubernetes cannot mount or manage EBS volumes for your pods, and this static PV + PVC setup will not work.

### EBS Volumes are AZ-Specific:
Amazon EBS volumes are tied to a specific **Availability Zone (AZ)**. You cannot attach a volume in `ap-south-2a` to a node in `ap-south-2b`.

Therefore, when using static provisioning with EBS, it's critical to:
- Create the EBS volume in the **same AZ** as the Kubernetes node that will mount it.
- Use `nodeAffinity` in the PersistentVolume to ensure the volume only gets attached to nodes in the correct AZ.

### Steps to Use Static Provisioning:
1. Create a PersistentVolume (PV) with node affinity
2. Create a PersistentVolumeClaim (PVC) that matches the PV
3. Mount the PVC inside the MongoDB pod

### üîç Verifying Node AZ Labels:
To check what zones your nodes are in:
```bash
kubectl get nodes --show-labels
```
Look for a label like:
```
topology.kubernetes.io/zone=ap-south-2a
```
If the label is missing or incorrect:
```bash
kubectl label node <node-name> topology.kubernetes.io/zone=ap-south-2a
```

### üîß How to Create an EBS Volume and Get Its ID (AWS):
1. Go to the **EC2 Dashboard** in AWS Console.
2. In the left sidebar, click **Volumes** under "Elastic Block Store".
3. Click **Create Volume**.
4. Choose the following options:
   - **Volume Type:** gp2 (or gp3)
   - **Size:** e.g., 1 GiB
   - **Availability Zone:** Must match your Kubernetes worker node AZ (e.g., `ap-south-2a`)
5. Click **Create Volume**.
6. After creation, note the **Volume ID** (e.g., `vol-047f6bbbe478b6075`).

‚û°Ô∏è This method ensures that the volume already exists before the pod uses it, making it a static provisioning setup.

### üì¶ PersistentVolume Example (with Node Affinity):
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongo-static-pv
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  csi:
    driver: ebs.csi.aws.com
    volumeHandle: vol-047f6bbbe478b6075  # Replace with your EBS Volume ID
    fsType: ext4
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - ap-south-2a
  storageClassName: ""
```

### üìÑ PersistentVolumeClaim:
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongo-static-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: ""
```
‚úÖ Kubernetes will automatically bind this PVC to the matching PV based on access mode, capacity, and empty storage class.

### üß± Deployment YAML:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-static
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-static
  template:
    metadata:
      labels:
        app: mongo-static
    spec:
      containers:
      - name: mongo
        image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: secret123
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        persistentVolumeClaim:
          claimName: mongo-static-pvc
```

‚û°Ô∏è **Result:**
Data is **persisted** even after pod deletion. As long as the EBS CSI Driver is installed, the volume is correctly bound, and the node affinity matches the node's AZ, your MongoDB data will remain intact.

### ‚ö†Ô∏è Pod Scheduling Behavior:
If no suitable node exists in the AZ specified in the PV's nodeAffinity, the pod will not be scheduled.

Example error:
```text
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  22s   default-scheduler  0/1 nodes are available: 1 node(s) had volume node affinity conflict. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
```

üîç What this means:
Kubernetes tried to schedule the pod, but none of the available nodes matched the volume's `nodeAffinity`. This usually means the pod cannot be scheduled because it can only use the volume on nodes in a specific AZ (e.g., `ap-south-2a`).

-  Ensure that your node pool includes at least one node in the specified AZ.
- Double-check volume AZ and node labels.
- Use `kubectl describe pod <pod-name>` to troubleshoot scheduling issues.

### üí° Tip:
- A PV can only be bound to **one PVC**.
- Even if the PVC requests less than the PV size, the full PV is **reserved**.
- Remaining space in the PV **cannot be reused** by other PVCs.
- To avoid waste, create smaller PVs that match your application's actual storage needs.


### üõ†Ô∏è Troubleshooting: PV Stuck in "Terminating" State
If a PVC is deleted before the PV and the `persistentVolumeReclaimPolicy` is set to `Retain`, the PV might get stuck in a `Terminating` state. This is due to Kubernetes waiting to perform clean-up defined by the `finalizers`.

#### üßπ Solution: Remove Finalizers from PV
To force delete the PV, run:
```bash
kubectl patch pv mongo-static-pv -p '{"metadata":{"finalizers":null}}' --type=merge
```
‚úÖ This will remove the finalizer and allow the PV to be deleted immediately.

üìå Use this only when you're sure the volume is no longer needed and safe to remove manually.


---

## üìÅ 4. PersistentVolumeClaim with Dynamic Volume Provisioning

### üöÄ What is Dynamic Provisioning?
- Dynamic provisioning allows Kubernetes to automatically create a PersistentVolume (PV) when a PersistentVolumeClaim (PVC) is made.
- No need to manually pre-create volumes (like in static provisioning).
- Great for cloud environments (AWS, GCP, Azure) where storage APIs are available.

### ‚öôÔ∏è Requirements:
- A working **StorageClass**.
- A **CSI Driver** (like AWS EBS CSI driver) must be installed.

> üß† **Note:** If you don‚Äôt define a `storageClassName` in your PVC, Kubernetes may use the default StorageClass.

---

### üì¶ StorageClass Example (for AWS EBS):
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: newstorageclass
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
```

### üìÑ PersistentVolumeClaim:
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongo-dynamic-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: newstorageclass
```

‚û°Ô∏è Kubernetes will use the `newstorageclass` StorageClass to provision a 1Gi volume. That means `pv - (ebs volume)` is going to create automatically whenever we request it.

---

### üß± Deployment Example:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-dynamic
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-dynamic
  template:
    metadata:
      labels:
        app: mongo-dynamic
    spec:
      containers:
      - name: mongo
        image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: secret123
        volumeMounts:
        - name: mongo-storage
          mountPath: /data/db
      volumes:
      - name: mongo-storage
        persistentVolumeClaim:
          claimName: mongo-dynamic-pvc
```

---

### ‚úÖ Result:
- Kubernetes provisions an EBS volume when PVC is created.
- Volume is automatically attached to the pod.
- When the PVC is deleted, the volume can also be deleted (depending on `reclaimPolicy`).

---

### üìò Tip:
- Dynamic provisioning saves time and reduces errors.
- Use it when you don‚Äôt want to manage storage manually.
- Works great with StatefulSets too.

---

### üßæ About `reclaimPolicy`
- The `reclaimPolicy` is specified inside the **StorageClass**.
- It controls what happens to the volume **after the PVC is deleted**.

#### Types of `reclaimPolicy`:
- `Delete` - Deletes the underlying storage automatically.
- `Retain` - Keeps the volume; you must delete it manually.
- `Recycle` - Deprecated and not supported in most environments.

> ‚úÖ **Recommended**: Use `Delete` for dynamic provisioning in most cloud-native use cases.


---

### ‚õìÔ∏è About `volumeBindingMode`
- `volumeBindingMode` defines **when** the PersistentVolume is bound to a PersistentVolumeClaim.

#### Types:
- `Immediate` (default):
  - The volume is provisioned as soon as the PVC is created.
  - May lead to pod scheduling problems if volume and pod are in different zones.

- `WaitForFirstConsumer`:
  - The volume is provisioned **only when a pod using the PVC is scheduled**.
  - Ensures that the volume is created in the **same Availability Zone** as the pod.

> üìå **Best Practice**: Use `WaitForFirstConsumer` in multi-AZ clusters to avoid volume and pod AZ mismatch issues.

---

### üìö About `accessModes`
The `accessModes` field defines **how many nodes can mount the volume at the same time**.

#### Types:
- `ReadWriteOnce` (RWO):
  - Can be mounted as read-write by **a single node**.
  - Most commonly used mode (especially for EBS).

- `ReadOnlyMany` (ROX):
  - Can be mounted as read-only by **many nodes** simultaneously.

- `ReadWriteMany` (RWX):
  - Can be mounted as read-write by **many nodes**.
  - Requires a networked filesystem (like NFS, or EFS on AWS).

> ‚ÑπÔ∏è For AWS EBS, only `ReadWriteOnce` is supported.


---

### üìå Common Troubleshooting:
- **PVC Pending**: Check if the StorageClass exists and the CSI driver is installed.
- **Pod not starting**: Check `kubectl describe pod <name>` for volume-related events.
- **Volume not created**: Check cloud permissions and controller logs.

> üì¶ Dynamic provisioning is cloud-native and DevOps-friendly!

---

### üêõ Multi-Attach Error in EBS Volumes (Kubernetes)
- AWS EBS volumes support only **ReadWriteOnce (RWO)** mode.
- That means an EBS volume **can only be attached to one node at a time**.

#### ‚ùå Scenario:
Suppose your pod is running with a dynamically provisioned volume, and the **node gets deleted** or crashes.
- Kubernetes will try to recreate the pod on a different node.
- However, the original pod may still be in `Terminating` state, and the volume is still considered attached to the old node.
- You'll see an error like:

```
Warning  FailedAttachVolume  attachdetach-controller  Multi-Attach error for volume "<pvc-id>": Volume is already used by pod(s) <pod-name>
```

#### üí° Solution:
- Kubernetes can‚Äôt detach and re-attach the volume until the old pod is fully terminated.
- To avoid this:
  - Use **StatefulSet** instead of Deployment for volume-bound workloads like databases.
  - StatefulSets have stable pod identity and handle volume lifecycle better.
  - Also consider setting a `terminationGracePeriodSeconds` to allow graceful pod shutdown.

---
### üõ†Ô∏è Troubleshooting Common Issues

| Problem | Possible Cause |
|--------|------------------|
| PVC stuck in `Pending` | No available StorageClass or provisioning failed |
| Pod stuck in `ContainerCreating` | Volume not yet attached or available |
| IAM access denied | Missing EC2 or EBS permissions |
| Multi-AZ errors | `volumeBindingMode` not set to `WaitForFirstConsumer` |
| Multi-Attach error | EBS volumes are RWO - cannot attach to multiple nodes |
| Node not ready | Taints or missing CSI driver on node |
| PVC bound but pod crashlooping | App-level error or volume permission issues |
| Node has volume node affinity conflict | Pod scheduled to different AZ from volume |

---

## ‚úÖ Summary Table
| Volume Type                  | Data Persisted After Pod Deletion? | Notes |
|-----------------------------|-------------------------------------|-------|
| emptyDir                    | ‚ùå No                              | Temporary for pod lifetime only |
| hostPath                    | ‚úÖ Yes (on same node)              | Not portable; data lost on different node |
| PVC (Static)                | ‚úÖ Yes                             | Manual setup of PV or default provisioner |
| PVC (Dynamic + StorageClass)| ‚úÖ Yes                             | Automatic volume management |

---
### üß† Can I Scale a Deployment That Uses a Single PVC?

- ‚ùå No, scaling a Deployment with `replicas > 1` using a single PVC backed by EBS won't work.
- AWS EBS volumes support `ReadWriteOnce` ‚Äî meaning they can be attached to only **one pod on one node**.
- If you try scaling such a Deployment, the additional pods will fail to start due to **Multi-Attach errors**.

---

### ‚úÖ Recommended Solution

- Use a **StatefulSet** with `volumeClaimTemplates`.
- Each replica gets a **unique PVC and EBS volume**.
- Ideal for databases like MongoDB that require per-instance storage.

---
---
---
### ü§î Why Use StatefulSet if Deployment + PVC Can Persist Data?

While **Deployments with PVCs** can persist data, they are still designed for **stateless** workloads.

Problems with Deployment for stateful apps:
- Pod names are **not stable**. Each new pod gets a new random name (like `app-5c7f9f8c7d-x9k7t`).
- PVCs are not automatically reused with the same pod.
- No **ordered startup or shutdown**.
- No stable **network identity** or hostname.

### ‚úÖ What StatefulSet Provides:
- **Stable, sticky identity** for each pod (`pod-0`, `pod-1`, etc.)
- **Consistent network hostname** (e.g., `mongo-0.mongo-headless.default.svc.cluster.local`)
- **Stable volume claim per pod** (e.g., `data-mongo-0`, `data-mongo-1`)
- **Ordered** deployment, scaling, and termination.

> So, use **StatefulSets** when your app needs stable identity, hostname, or storage across restarts and scaling.

üì¶ Examples:
- MongoDB replicaset pods
- MySQL primary/replica setup
- Kafka brokers

#### Comparison Table:
| Feature | Deployment + PVC | StatefulSet |
|--------|------------------|-------------|
| Data Persistence | ‚úÖ Yes | ‚úÖ Yes |
| Stable Network Identity | ‚ùå No | ‚úÖ Yes (`pod-0`, `pod-1`) |
| Stable Storage | ‚ö†Ô∏è Shared PVC | ‚úÖ One PVC per pod |
| Ordered Deployment | ‚ùå No | ‚úÖ Yes |
| Use case | Simple apps | DBs, Kafka, Zookeeper |

---

## üìÅ 5. StatefulSet + MongoDB ReplicaSet with PVCs

This setup deploys a MongoDB ReplicaSet using a Kubernetes StatefulSet, persistent volumes (via a custom `StorageClass`), and a headless service for stable network identity. This enables high availability and data persistence.

---

### üì¶ mongo-storageclass.yaml
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: newstorageclass
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  fsType: ext4
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
```

- **Purpose**: Creates a StorageClass named `newstorageclass` using the AWS EBS CSI driver.
- **WaitForFirstConsumer**: Ensures volumes are provisioned only after pod scheduling to avoid AZ mismatch.
- **Delete** policy: Delete the volume data after PVC deletion for safety.

---

### üîå mongo-headless-service.yaml
```yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo-statefulset-service
spec:
  ports:
    - port: 27017
      targetPort: 27017
  clusterIP: None
  selector:
    app: mongo-statefulset
```

- **Purpose**: Provides stable network identities for each MongoDB pod via DNS (`mongo-statefulset-0.mongo-statefulset-service`).
- **clusterIP: None**: Makes it headless to directly expose pod DNS.

---

### üß± mongo-statefulset.yaml
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo-statefulset
spec:
  serviceName: "mongo-statefulset-service"
  replicas: 3
  selector:
    matchLabels:
      app: mongo-statefulset
      environment: test
  template:
    metadata:
      labels:
        app: mongo-statefulset
        environment: test
    spec:
      terminationGracePeriodSeconds: 10
      containers:
        - name: mongo
          image: mongo:6.0
          command:
            - mongod
            - "--replSet"
            - rs0
            - "--bind_ip_all"
            - "--port"
            - "27017"
          ports:
            - containerPort: 27017
          volumeMounts:
            - name: mongo-statefulset-storage
              mountPath: /data/db
  volumeClaimTemplates:
    - metadata:
        name: mongo-statefulset-storage
      spec:
        storageClassName: "newstorageclass"
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 5Gi
```

- **command**: Ensures MongoDB starts in replica set mode.
- **StatefulSet**: Ensures stable pod identity and storage (e.g., `mongo-statefulset-0`).
- **PVC Template**: Requests 5Gi persistent volume for each pod using the `newstorageclass`.


---

### üîÅ ReplicaSet Initialization Steps

To form a MongoDB ReplicaSet inside a Kubernetes StatefulSet, we need to initialize and configure the set manually from one of the pods. Here's why and how:

1. **Exec into the first pod (`mongo-statefulset-0`)**:
   ```bash
   kubectl exec -it mongo-statefulset-0 -- bash
   ```
   - Since `mongo-statefulset-0` is the first pod (and always comes up first), it's ideal for initiating the replica set.

2. **Connect to the Mongo shell**:
   ```bash
   mongosh
   ```
   - This connects us to the MongoDB instance running on that pod.

3. **Initiate the replica set**:
   ```javascript
   rs.initiate({
     _id: "rs0",
     members: [
       { _id: 0, host: "mongo-statefulset-0.mongo-statefulset-service:27017" },
       { _id: 1, host: "mongo-statefulset-1.mongo-statefulset-service:27017" },
       { _id: 2, host: "mongo-statefulset-2.mongo-statefulset-service:27017" }
     ]
   })
   ```
   - This sets up the initial configuration of the ReplicaSet.
   - Uses the headless service `mongo-statefulset-service` to resolve pod names (`mongo-statefulset-0.mongo-statefulset-service`, etc.).
   - Ensures MongoDB knows all the members of the set.

4. **Check ReplicaSet status**:
   ```javascript
   rs.status()
   ```
   - Verifies that the ReplicaSet has been properly initialized.
   - You should see the state as `PRIMARY`, `SECONDARY`, etc., for the pods.

---
### üß† MongoDB ReplicaSet Behavior

- After running `rs.initiate()`, one pod becomes **Primary**, others become **Secondaries**.
- ‚úÖ Writes are only allowed on the Primary.
- ‚ùå By default, reads are not allowed on Secondaries ‚Äî but can be enabled.
- Use `rs.status()` to monitor the replica set.
- ReplicaSets ensure **high availability**, **failover**, and **replication** in a StatefulSet environment.

---
### StatefulSet and EBS Volume Multi-Attach Issue Explained

When using AWS EBS volumes with Kubernetes StatefulSets, it's important to understand how volume attachment and detachment behaves, especially during node failures or pod restarts.

### Scenario: Node Deleted, EBS Still Attached

When a node running a StatefulSet pod (e.g., `mongo-statefulset-0`) gets terminated (due to scaling down, an issue, or a node rotation), the following sequence can occur:

1. **EBS Volume Attachment Still Active:**
   - The EBS volume used by the StatefulSet pod remains in an **attached** state to the old (terminated) node.
   - This can be seen in the AWS console or CLI under EC2 ‚Üí Volumes ‚Üí Attached Instances.

2. **New Pod Scheduled on New Node:**
   - Kubernetes tries to reschedule the pod on a new node.
   - The pod attempts to attach the same persistent volume (PVC) to the new node.

3. **Multi-Attach Error Occurs:**
   - Since EBS volumes support only **single attachment** in `ReadWriteOnce` mode, the new node cannot attach the volume while it is still attached to the old (terminated) node.
   - This results in an event like:
     ```
     Multi-Attach error for volume "pvc-xxxxx" Volume is already exclusively attached to one node and can't be attached to another
     ```

4. **Automatic Resolution:**
   - After some time (usually a couple of minutes), AWS detects that the old node is gone and detaches the volume.
   - Kubernetes then retries the attach operation.
   - Once successful, the pod can start normally.

### Sample Events:
```shell
Normal   Scheduled               2m37s  default-scheduler        Successfully assigned default/mongo-statefulset-0 to ip-192-168-145-22.ap-south-2.compute.internal
Warning  FailedAttachVolume      2m37s  attachdetach-controller  Multi-Attach error for volume "pvc-xxxxx"
Warning  FailedMount             34s    kubelet                  timed out waiting for the condition
Normal   SuccessfulAttachVolume  31s    attachdetach-controller  AttachVolume.Attach succeeded
```

### Solutions and Best Practices:

#### üîç Find Where the Volume is Attached
You can use AWS CLI or Console to check the volume attachment:
```bash
aws ec2 describe-volumes --volume-ids <volume-id>
```
This shows which instance the volume is currently attached to.

#### üßπ Detach the Volume Manually
If the volume is stuck in an `in-use` state:
```bash
aws ec2 detach-volume --volume-id <volume-id>
```

#### üí£ Force Delete the Pod
If the pod is stuck in terminating state and holding on to the volume:
```bash
kubectl delete pod <pod-name> --grace-period=0 --force
```
This immediately kills the pod and allows the volume to be released.

---

### Summary
- EBS volumes take time to detach from terminated nodes.
- New pods can't attach the same volume until it's fully detached.
- Kubernetes resolves this automatically, but you can speed it up with manual intervention.
- Monitor events and logs for attach/detach issues.

These behaviors are normal but critical to understand when using **StatefulSets** and **EBS** with Kubernetes on AWS.

---

### ‚úÖ Benefits:
- Dedicated PVC per pod ensures safe failover
- ReplicaSet works with consistent pod hostnames
- Sidecar auto-handles replica member addition
- Best suited for production MongoDB clusters